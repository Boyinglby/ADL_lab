{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNteNrXFYOBMFWbmiADpmjq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boyinglby/ADL_lab/blob/main/DL_lab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "import numpy as np\n",
        "from matplotlib import pyplot\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk import word_tokenize\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, classification_report"
      ],
      "metadata": {
        "id": "Ts5rnSUZHHQ1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.1 Simple chatbot"
      ],
      "metadata": {
        "id": "W1NRSKHI034p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and preprocess data"
      ],
      "metadata": {
        "id": "SHlaDAVvFoxb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vecgo9StF8YY"
      },
      "outputs": [],
      "source": [
        "def preprocess_pandas(data, columns):\n",
        "    df_ = pd.DataFrame(columns=columns)\n",
        "    data['Sentence'] = data['Sentence'].str.lower()\n",
        "    data['Sentence'] = data['Sentence'].replace('[a-zA-Z0-9-_.]+@[a-zA-Z0-9-_.]+', '', regex=True)                      # remove emails\n",
        "    data['Sentence'] = data['Sentence'].replace('((25[0-5]|2[0-4][0-9]|[01]?[0-9][0-9]?)(\\.|$)){4}', '', regex=True)    # remove IP address\n",
        "    data['Sentence'] = data['Sentence'].str.replace('[^\\w\\s]','')                                                       # remove special characters\n",
        "    data['Sentence'] = data['Sentence'].replace('\\d', '', regex=True)                                                   # remove numbers\n",
        "    for index, row in data.iterrows():\n",
        "        word_tokens = word_tokenize(row['Sentence'])\n",
        "        filtered_sent = [w for w in word_tokens if not w in stopwords.words('english')]\n",
        "        df_ = pd.concat([df_, pd.DataFrame({\n",
        "            \"index\": row['index'],\n",
        "            \"Class\": row['Class'],\n",
        "            \"Sentence\": \" \".join(filtered_sent[0:])\n",
        "        }, index = [row['index']])],ignore_index=True)\n",
        "    return df_"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# get data, pre-process and split\n",
        "data = pd.read_csv(\"amazon_cells_labelled.txt\", delimiter='\\t', header=None)\n",
        "data.columns = ['Sentence', 'Class']\n",
        "data['index'] = data.index                                          # add new column index\n",
        "columns = ['index', 'Class', 'Sentence']"
      ],
      "metadata": {
        "id": "gUJ7870nG5XP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the NLTK library\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "data = preprocess_pandas(data, columns)                             # pre-process\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4N7c4qRIGA2",
        "outputId": "a04dd749-9cb3-4557-ecf6-eb8f3ab9a9f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data_r, validation_data_r, training_labels, validation_labels = train_test_split( # split the data into training, validation, and test splits\n",
        "        data['Sentence'].values.astype('U'),\n",
        "        data['Class'].values.astype('int32'),\n",
        "        test_size=0.10,\n",
        "        random_state=0,\n",
        "        shuffle=True\n",
        "    )"
      ],
      "metadata": {
        "id": "7UOkYJVsIeq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# vectorize data using TFIDF and transform for PyTorch for scalability\n",
        "word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,2), max_features=50000, max_df=0.5, use_idf=True, norm='l2')\n",
        "training_data = word_vectorizer.fit_transform(training_data_r)        # transform texts to sparse matrix\n",
        "training_data = training_data.todense()                             # convert to dense matrix for Pytorch\n",
        "\n",
        "vocab_size = len(word_vectorizer.vocabulary_)\n",
        "\n",
        "validation_data = word_vectorizer.transform(validation_data_r)\n",
        "validation_data = validation_data.todense()\n",
        "\n",
        "train_x_tensor = torch.from_numpy(np.array(training_data)).type(torch.FloatTensor)\n",
        "train_y_tensor = torch.from_numpy(np.array(training_labels)).long()\n",
        "\n",
        "validation_x_tensor = torch.from_numpy(np.array(validation_data)).type(torch.FloatTensor)\n",
        "validation_y_tensor = torch.from_numpy(np.array(validation_labels)).long()"
      ],
      "metadata": {
        "id": "ImZ9MIsBPNts"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LSTM model"
      ],
      "metadata": {
        "id": "Ah7dtSuDFlT4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import random\n",
        "\n",
        "out_nr = 2\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, hidden_size = 25):\n",
        "        super(LSTM, self).__init__()\n",
        "        num_layers = 1\n",
        "        # Recurrent layer\n",
        "        self.lstm = nn.LSTM(input_size=vocab_size,\n",
        "                         hidden_size=hidden_size,\n",
        "                         num_layers=num_layers,\n",
        "                         bidirectional=False)\n",
        "\n",
        "        # Output layer\n",
        "        self.l_out = nn.Linear(in_features=hidden_size,\n",
        "                            out_features=out_nr,\n",
        "                            bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # RNN returns output and last hidden state\n",
        "        x, (h, c) = self.lstm(x)\n",
        "\n",
        "        # Flatten output for feed-forward layer\n",
        "        x = x.view(-1, self.lstm.hidden_size)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.l_out(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def generate_resp(self,x):\n",
        "      response = {0:[\"sorry for the bad experience\", \"please contact the service for help\", \"We are improving\"],\n",
        "                  1:[\"Thank you for the feedback\", \"we are glad you like it\"]}\n",
        "      output = self.forward(x)\n",
        "      analysis = np.argmax(output[0].detach().numpy())\n",
        "      print(random.choice(response[analysis]))\n",
        "\n"
      ],
      "metadata": {
        "id": "tkyZ8WUrFGWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise our network\n",
        "model = LSTM()\n",
        "print(model)\n",
        "min_LSTM_loss = 10000\n",
        "\n",
        "# Define a loss function and optimizer for this problem\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# criterion = nn.BCELoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=3e-3)\n",
        "# A way to get learning rate decay\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pDZaazAOGCfV",
        "outputId": "00414211-f9e3-4089-afff-c20e14620c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LSTM(\n",
            "  (lstm): LSTM(5145, 25)\n",
            "  (l_out): Linear(in_features=25, out_features=2, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# one hot encode label\n",
        "to_onehot = nn.Embedding(1, 2)\n",
        "to_onehot.weight.data = torch.eye(2)"
      ],
      "metadata": {
        "id": "NBIcj5zFXR8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyper-parameters\n",
        "num_epochs = 5\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# Store training and validation loss\n",
        "training_loss, validation_loss = [], []\n",
        "# For each epoch\n",
        "for i in range(num_epochs):\n",
        "\n",
        "    # Track loss\n",
        "    epoch_training_loss = 0\n",
        "    epoch_validation_loss = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    # For each sentence in validation set\n",
        "    correct = 0\n",
        "    for n in range(len(validation_x_tensor)):\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model.forward(validation_x_tensor[n].reshape(1,vocab_size))\n",
        "\n",
        "        # accuracy\n",
        "        label = validation_y_tensor[n].item()\n",
        "\n",
        "        if np.argmax(outputs[0].detach().numpy()) == label:\n",
        "          correct += 1\n",
        "\n",
        "        loss = criterion(outputs[0], to_onehot(validation_y_tensor[n]))\n",
        "\n",
        "        # Update loss\n",
        "        epoch_validation_loss += loss.detach().numpy()\n",
        "    correct = correct /len(validation_x_tensor)\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # For each sentence in training set\n",
        "    for m in range(len(train_x_tensor)):\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model.forward(train_x_tensor[m].reshape(1,vocab_size))\n",
        "\n",
        "        # Compute loss\n",
        "\n",
        "        loss = criterion(outputs[0], to_onehot(train_y_tensor[m]))\n",
        "\n",
        "        # Reset gradients\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        # Compute gradients\n",
        "\n",
        "        loss.backward()\n",
        "        # Update weights\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update learning rate (advanced technique, can be ignored)\n",
        "        scheduler.step()\n",
        "\n",
        "        # Update loss\n",
        "        epoch_training_loss += loss.detach().numpy()\n",
        "\n",
        "    # Save loss for plot\n",
        "    training_loss.append(epoch_training_loss/len(train_x_tensor))\n",
        "    validation_loss.append(epoch_validation_loss/len(validation_x_tensor))\n",
        "\n",
        "    # save best model\n",
        "    if epoch_validation_loss < min_LSTM_loss:\n",
        "        torch.save(model, 'best_model.pt')\n",
        "    # Print loss every 5 epochs\n",
        "    if i % 1 == 0:\n",
        "        print(f'Epoch {i}, training loss: {training_loss[-1]}, validation loss: {validation_loss[-1]}, validation accuracy: {correct*100}%')\n",
        "\n",
        "# Plot training and validation loss\n",
        "epoch = np.arange(len(training_loss))\n",
        "\n",
        "# Forward pass\n",
        "best_model = LSTM()\n",
        "best_model = torch.load('best_model.pt')"
      ],
      "metadata": {
        "id": "Wcul7eD8GDT2",
        "outputId": "2101229f-bcf3-4e1e-9cfe-32fa86c96775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0, training loss: 0.6810181766417291, validation loss: 0.6940236765146256, validation accuracy: 44.0%\n",
            "Epoch 1, training loss: 0.6394979037510025, validation loss: 0.6519566321372986, validation accuracy: 81.0%\n",
            "Epoch 2, training loss: 0.6346127455764347, validation loss: 0.6434415152668953, validation accuracy: 80.0%\n",
            "Epoch 3, training loss: 0.6344017678830358, validation loss: 0.6430730104446412, validation accuracy: 80.0%\n",
            "Epoch 4, training loss: 0.6343935613168611, validation loss: 0.6430580839514732, validation accuracy: 80.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss curve\n",
        "plt.figure()\n",
        "plt.plot(epoch, training_loss, 'r', label='Training loss',)\n",
        "plt.plot(epoch, validation_loss, 'b', label='Validation loss')\n",
        "plt.legend()\n",
        "plt.xlabel('Epoch'), plt.ylabel('NLL')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "xcunZPedZPTS",
        "outputId": "7ff175e9-5220-46a9-f7c1-6e78454102d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get first sentence in validation set\n",
        "inputs, targets = validation_x_tensor[0], validation_y_tensor[0]\n",
        "outputs = best_model.forward(inputs.reshape(1,vocab_size)).data.numpy()\n",
        "\n",
        "print('\\nInput sequence:')\n",
        "print(validation_data_r[0])\n",
        "\n",
        "print('\\nTarget sequence:')\n",
        "print(targets)\n",
        "\n",
        "preds = np.argmax(outputs)\n",
        "print('\\nPredicted sequence:')\n",
        "print(preds)\n",
        "\n",
        "print('\\nChatbot response:')\n",
        "best_model.generate_resp(inputs.reshape(1,vocab_size))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WUeYrzDGaJlO",
        "outputId": "680090b8-9299-441a-a952-ab69b5cbe403"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Input sequence:\n",
            "disappointed .\n",
            "\n",
            "Target sequence:\n",
            "tensor(0)\n",
            "\n",
            "Predicted sequence:\n",
            "0\n",
            "\n",
            "Chatbot response:\n",
            "We are improving\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.2 Transformer"
      ],
      "metadata": {
        "id": "hXJH1Qzc0_Sb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Refer to https://www.youtube.com/watch?v=kCc8FmEb1nY"
      ],
      "metadata": {
        "id": "NujLulMiBO_X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 16 # how many independent sequences will we process in parallel?\n",
        "block_size = 32 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-3\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 64\n",
        "n_head = 4\n",
        "n_layer = 4\n",
        "dropout = 0.0"
      ],
      "metadata": {
        "id": "IyyNu88c1GTU"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "clw8Ixes1brO",
        "outputId": "0bdc38d1-76d6-4d01-f0b3-9a9aa8c5a05e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-04-17 13:13:19--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2024-04-17 13:13:20 (19.8 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(train_data)"
      ],
      "metadata": {
        "id": "E2EyqK1tDE-t",
        "outputId": "9da72d06-4b56-4a28-d167-08c91ff587bf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1003854"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "# super simple bigram model\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n"
      ],
      "metadata": {
        "id": "emKh0Sss1n4r"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "id": "N-xXgZ3b2PX4",
        "outputId": "d593dd24-9e92-4caf-f9c3-1c74667a84f9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.209729 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "sOZX5oJ71u3g",
        "outputId": "8f9af305-1b28-40f0-96d7-01c27ed842a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.4116, val loss 4.4022\n",
            "step 100: train loss 2.6568, val loss 2.6670\n",
            "step 200: train loss 2.5090, val loss 2.5058\n",
            "step 300: train loss 2.4194, val loss 2.4335\n",
            "step 400: train loss 2.3503, val loss 2.3567\n",
            "step 500: train loss 2.2964, val loss 2.3131\n",
            "step 600: train loss 2.2404, val loss 2.2494\n",
            "step 700: train loss 2.2043, val loss 2.2177\n",
            "step 800: train loss 2.1629, val loss 2.1856\n",
            "step 900: train loss 2.1240, val loss 2.1500\n",
            "step 1000: train loss 2.1015, val loss 2.1286\n",
            "step 1100: train loss 2.0685, val loss 2.1178\n",
            "step 1200: train loss 2.0382, val loss 2.0792\n",
            "step 1300: train loss 2.0251, val loss 2.0637\n",
            "step 1400: train loss 1.9940, val loss 2.0377\n",
            "step 1500: train loss 1.9691, val loss 2.0310\n",
            "step 1600: train loss 1.9631, val loss 2.0481\n",
            "step 1700: train loss 1.9402, val loss 2.0124\n",
            "step 1800: train loss 1.9091, val loss 1.9958\n",
            "step 1900: train loss 1.9069, val loss 1.9857\n",
            "step 2000: train loss 1.8835, val loss 1.9925\n",
            "step 2100: train loss 1.8699, val loss 1.9730\n",
            "step 2200: train loss 1.8584, val loss 1.9597\n",
            "step 2300: train loss 1.8541, val loss 1.9527\n",
            "step 2400: train loss 1.8412, val loss 1.9423\n",
            "step 2500: train loss 1.8159, val loss 1.9411\n",
            "step 2600: train loss 1.8222, val loss 1.9347\n",
            "step 2700: train loss 1.8097, val loss 1.9322\n",
            "step 2800: train loss 1.8011, val loss 1.9174\n",
            "step 2900: train loss 1.8024, val loss 1.9276\n",
            "step 3000: train loss 1.7929, val loss 1.9140\n",
            "step 3100: train loss 1.7686, val loss 1.9183\n",
            "step 3200: train loss 1.7549, val loss 1.9115\n",
            "step 3300: train loss 1.7564, val loss 1.9077\n",
            "step 3400: train loss 1.7538, val loss 1.8954\n",
            "step 3500: train loss 1.7379, val loss 1.8958\n",
            "step 3600: train loss 1.7259, val loss 1.8883\n",
            "step 3700: train loss 1.7280, val loss 1.8791\n",
            "step 3800: train loss 1.7193, val loss 1.8886\n",
            "step 3900: train loss 1.7200, val loss 1.8695\n",
            "step 4000: train loss 1.7137, val loss 1.8615\n",
            "step 4100: train loss 1.7141, val loss 1.8754\n",
            "step 4200: train loss 1.7045, val loss 1.8624\n",
            "step 4300: train loss 1.7004, val loss 1.8476\n",
            "step 4400: train loss 1.7074, val loss 1.8726\n",
            "step 4500: train loss 1.6920, val loss 1.8539\n",
            "step 4600: train loss 1.6896, val loss 1.8382\n",
            "step 4700: train loss 1.6828, val loss 1.8444\n",
            "step 4800: train loss 1.6671, val loss 1.8474\n",
            "step 4900: train loss 1.6731, val loss 1.8416\n",
            "step 4999: train loss 1.6671, val loss 1.8286\n",
            "\n",
            "ROMEO:\n",
            "But you fret, hell, whereIV: it they to duke I usprocely fittle offect I'll wanter.\n",
            "\n",
            "BONDINGHNOR:\n",
            "You! but usom upwor your myst gliman;\n",
            "This Iell thre sure hanch them nour gruars:\n",
            "Now to by betwereight, tow, go:\n",
            "Eve not, Doh souls shall; them Those not.\n",
            "\n",
            "LUCIO:\n",
            "Lord,----\n",
            "But thou sging them this my freceim\n",
            "sear\n",
            "By Look's to; contray\n",
            "futwair art sade but grove to him, he's subdore in that lest.\n",
            "\n",
            "GLOUCESTER:\n",
            "Faway, as I neat.\n",
            "\n",
            "MENRIZERENB:\n",
            "Olving this swame, but strave so grain.\n",
            "\n",
            "MENENIUS:\n",
            "Geniry's bigguardsly come;\n",
            "Haid trues forsweet, sirr.\n",
            "\n",
            "POMPEREY:\n",
            "These stonge toe as passon\n",
            "The from throw and potater?\n",
            "\n",
            "Secome:\n",
            "Henrath my in fortunue: my latchmant I late all that you by joyly us belt slinem.\n",
            "\n",
            "KING-\n",
            "FRUKHESS My shalt juntance: that you, prinring Afflorceling\n",
            "What colforth temine you wast shoult where forpend,\n",
            "And-finus I'll that confect I come,\n",
            "But; man.\n",
            "\n",
            "BRUCKINGHASS:\n",
            "On you, argive, with surnes diphnsbreath\n",
            "Herefare, name a dought, thou\n",
            "Then fathers mades exety with.\n",
            "\n",
            "GLOUCESTER:\n",
            "Fixe.\n",
            "Is I would madie, no an it that would patch evile turn than war.\n",
            "\n",
            "LUCENTIO:\n",
            "I usquelch here crovides.\n",
            "\n",
            "PUARIEE:\n",
            "It that advoutly since, and marry.\n",
            "Mut wondy worse is senst my long.\n",
            "\n",
            "YORYVUPURENCELIO:\n",
            "Yest humast of the loves my lord:\n",
            "New, his stort not his morst.\n",
            "You more, to\n",
            "nom a rathing if thyseal,\n",
            "On think, if by yold, thou how onferds frulling.\n",
            "\n",
            "RICHARD:\n",
            "The steam, it you:\n",
            "Onfull him it no ars. I so gettles,\n",
            "I ande such the cruntion the rest:\n",
            "My brive, he get belaw, with the his say\n",
            "as towns, pressions: you lom if curner,\n",
            "But unter, toges, by aft, fantes, Bety on the,\n",
            "Pronge unteett, thou make-banited?\n",
            "\n",
            "LORD GAURET:\n",
            "These what reagt say mover a desbriek which o' an the;\n",
            "O, you to\n",
            "mellike they us thus usspried with requmate,\n",
            "All leats, bretcius unfeece that I would back with wowith you wind you,\n",
            "My leave it. Delporte these, be to stless this high pray poolaful brittles sure make Destiture,\n",
            "At not yet have contress. Thou prant!\n",
            "O,' you goligh you. Come flippown,\n",
            "And fatt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task 1.3 modify the Chatbot"
      ],
      "metadata": {
        "id": "1XzfnTSVA8g4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LSTM(nn.Module):\n",
        "    def __init__(self, hidden_size = 25):\n",
        "        super(LSTM, self).__init__()\n",
        "        num_layers = 1\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        # Recurrent layer\n",
        "        self.lstm = nn.LSTM(input_size=vocab_size,\n",
        "                         hidden_size=hidden_size,\n",
        "                         num_layers=num_layers,\n",
        "                         bidirectional=False,\n",
        "                         batch_first=True)\n",
        "\n",
        "        # Output layer\n",
        "        self.l_out = nn.Linear(in_features=hidden_size,\n",
        "                            out_features=vocab_size, # out_features modified from 2 (binary classification) to vocab_size\n",
        "                            bias=False)\n",
        "\n",
        "    def forward(self, x, targets=None):\n",
        "        # RNN returns output and last hidden state\n",
        "\n",
        "        x = self.token_embedding_table(x)\n",
        "\n",
        "\n",
        "        x, (h, c) = self.lstm(x)\n",
        "        # Output layer\n",
        "        x = self.l_out(x)\n",
        "        B, S, C = x.shape\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          x = x.view(B*S, C)\n",
        "          targets = targets.view(B*S)\n",
        "          loss = F.cross_entropy(x, targets)\n",
        "        return x, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "08hb-UZwBF8S"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = LSTM()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')"
      ],
      "metadata": {
        "id": "R38ZrWLVGC6Q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cd034c27-3149-48f2-dacf-a13f098d4175"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.014985 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "eval_interval = 200\n",
        "\n",
        "for iter in range(max_iters):\n",
        "      # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n"
      ],
      "metadata": {
        "id": "WsjgjILaGIcq",
        "outputId": "5f98b0be-cc70-4813-c0c9-4691a8717221",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.1846, val loss 4.1852\n",
            "step 200: train loss 2.8770, val loss 2.8831\n",
            "step 400: train loss 2.5679, val loss 2.5712\n",
            "step 600: train loss 2.4421, val loss 2.4487\n",
            "step 800: train loss 2.3760, val loss 2.3792\n",
            "step 1000: train loss 2.3293, val loss 2.3447\n",
            "step 1200: train loss 2.2939, val loss 2.3063\n",
            "step 1400: train loss 2.2721, val loss 2.2899\n",
            "step 1600: train loss 2.2468, val loss 2.2541\n",
            "step 1800: train loss 2.2113, val loss 2.2353\n",
            "step 2000: train loss 2.1967, val loss 2.2105\n",
            "step 2200: train loss 2.1798, val loss 2.2026\n",
            "step 2400: train loss 2.1647, val loss 2.1880\n",
            "step 2600: train loss 2.1492, val loss 2.1763\n",
            "step 2800: train loss 2.1363, val loss 2.1695\n",
            "step 3000: train loss 2.1254, val loss 2.1523\n",
            "step 3200: train loss 2.0989, val loss 2.1508\n",
            "step 3400: train loss 2.0957, val loss 2.1416\n",
            "step 3600: train loss 2.0979, val loss 2.1349\n",
            "step 3800: train loss 2.0809, val loss 2.1321\n",
            "step 4000: train loss 2.0803, val loss 2.1231\n",
            "step 4200: train loss 2.0726, val loss 2.1207\n",
            "step 4400: train loss 2.0619, val loss 2.1109\n",
            "step 4600: train loss 2.0532, val loss 2.1125\n",
            "step 4800: train loss 2.0470, val loss 2.0980\n",
            "step 4999: train loss 2.0407, val loss 2.0920\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=2000)[0].tolist()))"
      ],
      "metadata": {
        "id": "rtl6_o4iuRqF",
        "outputId": "39fa8763-256a-4a24-ae02-0400821067de",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "My Tourd to he mantecnctir shiosh nos and the by's thasbathory hese.\n",
            "\n",
            "QUEPEZIO:\n",
            "\n",
            "JUKAELY:\n",
            "Men ond a not a perd\n",
            "st;\n",
            "And sution\n",
            "suntut\n",
            "and the Gare. you.\n",
            "\n",
            "HORENTIZIO:\n",
            "For hy for onreilt ose ther, eapachers, this the u's sastly Anding? Will'd to me\n",
            "And prike whlald serinn'll he our moneiebimpe of arords; Srind nit.\n",
            "\n",
            "COMIOLIRY:\n",
            "I fore speark mell me ton tarn\n",
            "Ind wen this and hit, onofe a doud\n",
            "Siliote\n",
            "Not cobt has were blaiosel-of crajestath\n",
            "Or boed my tharldhath rvereing eat! itistarn thomuch in bows,\n",
            "brays bent to mayerst, lite utaiw then whacyere arick, Hord, ond ats thin an dem, nous my dy.\n",
            "\n",
            "No Ill senst siling, broveed\n",
            "Prord the thand, hastiras whorr mast hee protherch lot dot rilice:\n",
            "Hy pot dithen:\n",
            "of and\n",
            "Hore, ome, thy sir, whts anthy to fath\n",
            "Bou tronter hin the wiml; Boy thmunse.\n",
            "\n",
            "GLUKE LARENND CANCUS:\n",
            "Ther; me uplocrither torh kid;\n",
            "\n",
            "KCARDI3B ebai: dormayent thet, hear,\n",
            "Wronce aqufrord wiml our nose If on heath thase Gokefemsom!\n",
            "No be in so. I rone yo wimade, the notrentoup mes the deaghing,\n",
            "in thime thing heer hees hich. erscied niveest crome sharash.\n",
            "\n",
            "GREOWALIUS:\n",
            "\n",
            "Thaf most\n",
            "Ased sht I it as sering,\n",
            "Ton he I the plater the nongan to the voarsty's thco'd\n",
            "Of hom his fing gimes, to pirshant, all wno sow his yas win\n",
            "sharst as to moust\n",
            "And here op.\n",
            "\n",
            "PORWRI:\n",
            "Dospurtpengeathan\n",
            "soul the shou thing and fare bijestroorch she ilt! Morro me my faot son titter.  one wis he greby thou gere josdine for\n",
            "We spiome!\n",
            "Theres.\n",
            "\n",
            "KINDBY:\n",
            "Your meen whlat greal!\n",
            "\n",
            "HOFhorf:\n",
            "Ay.\n",
            "\n",
            "Seccill heard? hane\n",
            "To mrondspil thow thwing nit by tour his my Ay\n",
            "fatoged my mirt, grair, feal?\n",
            "\n",
            "Col gravinst I mume a gof wark, by thind chip his theefter maond them;\n",
            "Sere I ild noth wourt my so I make, me the lis am daknes, in Wan lodak conde now\n",
            "o tupengen, masgeve and poed\n",
            "Gore to lay me trepue dother fachistle tlamen,\n",
            "The with beem of Sir hobtill with! I dary kins, and nother did to glokkest dow truchsle,\n",
            "O old your, yof usty hety seakre id I his ilk sin shere suue do, I  kof frin your to hithe.\n",
            "Womisirar, o\n"
          ]
        }
      ]
    }
  ]
}