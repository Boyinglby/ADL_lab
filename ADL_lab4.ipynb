{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOAP8IGLHfrly0++7b+hCZE",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Boyinglby/ADL_lab/blob/main/ADL_lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a2JF621UPqs3",
        "outputId": "110449c2-dcc6-4483-d5f6-47e67f6b75ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python 3.10.12\n"
          ]
        }
      ],
      "source": [
        "!python --version"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "g1zbS2xgPvG3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym[atari]\n",
        "!pip install ale_py\n",
        "!pip install autorom[accept-rom-license]"
      ],
      "metadata": {
        "id": "QsymfvpLR_Xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import time\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "# Imports all our hyperparameters from the other file\n",
        "from hyperparams import Hyperparameters as params\n",
        "\n",
        "# stable_baselines3 have wrappers that simplifies\n",
        "# the preprocessing a lot, read more about them here:\n",
        "# https://stable-baselines3.readthedocs.io/en/master/common/atari_wrappers.html\n",
        "from stable_baselines3.common.atari_wrappers import (\n",
        "    ClipRewardEnv,\n",
        "    EpisodicLifeEnv,\n",
        "    FireResetEnv,\n",
        "    MaxAndSkipEnv,\n",
        "    NoopResetEnv,\n",
        ")\n",
        "from stable_baselines3.common.buffers import ReplayBuffer\n",
        "\n",
        "\n",
        "# Creates our gym environment and with all our wrappers.\n",
        "def make_env(env_id, seed, idx, capture_video, run_name):\n",
        "    def thunk():\n",
        "        env = gym.make(env_id)\n",
        "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
        "        if capture_video:\n",
        "            if idx == 0:\n",
        "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
        "        env = NoopResetEnv(env, noop_max=30)\n",
        "        env = MaxAndSkipEnv(env, skip=4)\n",
        "        env = EpisodicLifeEnv(env)\n",
        "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
        "            env = FireResetEnv(env)\n",
        "        env = ClipRewardEnv(env)\n",
        "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
        "        env = gym.wrappers.GrayScaleObservation(env)\n",
        "        env = gym.wrappers.FrameStack(env, 4)\n",
        "        env.seed(seed)\n",
        "        env.action_space.seed(seed)\n",
        "        env.observation_space.seed(seed)\n",
        "        return env\n",
        "\n",
        "    return thunk\n",
        "\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, env):\n",
        "        super().__init__()\n",
        "        # TODO: Deinfe your network (agent)\n",
        "        # Look at Section 4.1 in the paper for help: https://arxiv.org/pdf/1312.5602v1.pdf\n",
        "\n",
        "        self.network = nn.Sequential(\n",
        "            nn.Conv2d(4, 16, 8, stride=4),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(16, 32, 4, stride=2),\n",
        "            nn.ReLU(),\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(9*9*32, 512), # (((84-8)/4+1)-4)/2+1 = 9\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, env.single_action_space.n)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.network(x / 255.0)\n",
        "\n",
        "\n",
        "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
        "    slope = (end_e - start_e) / duration\n",
        "    return max(slope * t + start_e, end_e)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    run_name = f\"{params.env_id}__{params.exp_name}__{params.seed}__{int(time.time())}\"\n",
        "\n",
        "    random.seed(params.seed)\n",
        "    np.random.seed(params.seed)\n",
        "    torch.manual_seed(params.seed)\n",
        "    torch.backends.cudnn.deterministic = params.torch_deterministic\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # env setup\n",
        "    envs = gym.vector.SyncVectorEnv([make_env(params.env_id, params.seed, 0, params.capture_video, run_name)])\n",
        "    assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
        "\n",
        "    q_network = QNetwork(envs).to(device)\n",
        "    optimizer = optim.Adam(q_network.parameters(), lr=params.learning_rate)\n",
        "    target_network = QNetwork(envs).to(device)\n",
        "    target_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    # Weâ€™ll be using experience replay memory for training our DQN.\n",
        "    # It stores the transitions that the agent observes, allowing us to reuse this data later.\n",
        "    # By sampling from it randomly, the transitions that build up a batch are decorrelated.\n",
        "    # It has been shown that this greatly stabilizes and improves the DQN training procedure.\n",
        "    rb = ReplayBuffer(\n",
        "        params.buffer_size,\n",
        "        envs.single_observation_space,\n",
        "        envs.single_action_space,\n",
        "        device,\n",
        "        optimize_memory_usage=False,\n",
        "        handle_timeout_termination=True,\n",
        "    )\n",
        "\n",
        "    obs = envs.reset()\n",
        "    for global_step in range(params.total_timesteps):\n",
        "        # Here we get epsilon for our epislon greedy.\n",
        "        epsilon = linear_schedule(params.start_e, params.end_e, params.exploration_fraction * params.total_timesteps, global_step)\n",
        "\n",
        "        # epsilon-greedy control the balance between exploration and exploitation\n",
        "        # It allows the agent to explore and learn more about the environment.\n",
        "        # Over time, as the agent accumulates better estimates of action values, it becomes more selective (greedy) in its choices.\n",
        "        if random.random() < epsilon:\n",
        "            actions = envs.action_space.sample()# TODO: sample a random action from the environment\n",
        "        else:\n",
        "            q_values = q_network(torch.tensor(obs))# TODO: get q_values from the network you defined, what should the network receive as input?\n",
        "            actions = torch.argmax(q_values, dim=1).cpu().numpy()\n",
        "\n",
        "        # Take a step in the environment\n",
        "        next_obs, rewards, dones, infos = envs.step(actions)\n",
        "\n",
        "        # Here we print our reward.\n",
        "        for info in infos:\n",
        "            if \"episode\" in info.keys():\n",
        "                print(f\"global_step={global_step}, episodic_return={info['episode']['r']}\")\n",
        "                break\n",
        "\n",
        "        # Save data to replay buffer\n",
        "        real_next_obs = next_obs.copy()\n",
        "        for idx, d in enumerate(dones):\n",
        "            if d:\n",
        "                real_next_obs[idx] = infos[idx][\"terminal_observation\"]\n",
        "\n",
        "        # Here we store the transitions in D\n",
        "        rb.add(obs, real_next_obs, actions, rewards, dones, infos)\n",
        "\n",
        "        obs = next_obs\n",
        "        # Training\n",
        "        if global_step > params.learning_starts:\n",
        "            if global_step % params.train_frequency == 0:\n",
        "                # Sample random minibatch of transitions from D\n",
        "                data = rb.sample(params.batch_size)\n",
        "                # You can get data with:\n",
        "                # data.observation, data.rewards, data.dones, data.actions\n",
        "\n",
        "                with torch.no_grad():\n",
        "                    # Now we calculate the y_j for non-terminal phi.\n",
        "                    target_max, _ = q_network(data.real_next_obs).max(1)# TODO: Calculate max Q\n",
        "                    td_target = data.rewards + params.gamma * target_max * (1 - data.dones)# TODO: Calculate the td_target (y_j)\n",
        "                                                                                          # Bellman equation Q = R + gamma*Q_next_max\n",
        "                old_val = q_network(data.obs).gather(1, data.actions).squeeze()\n",
        "                loss = F.mse_loss(old_val, td_target) # we want Q estimates the optimal policy\n",
        "\n",
        "                # perform our gradient decent step\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            # update target network\n",
        "            if global_step % params.target_network_frequency == 0:\n",
        "                for target_network_param, q_network_param in zip(target_network.parameters(), q_network.parameters()):\n",
        "                    target_network_param.data.copy_(\n",
        "                        params.tau * q_network_param.data + (1.0 - params.tau) * target_network_param.data\n",
        "                    )\n",
        "\n",
        "    if params.save_model:\n",
        "        model_path = f\"runs/{run_name}/{params.exp_name}_model\"\n",
        "        torch.save(q_network.state_dict(), model_path)\n",
        "        print(f\"model saved to {model_path}\")\n",
        "\n",
        "    envs.close()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zeXdrI9vUO8S",
        "outputId": "4acee4b3-4c2d-4b61-c6cc-a884400289b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/utils/seeding.py:138: DeprecationWarning: \u001b[33mWARN: Function `hash_seed(seed, max_bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/seeding.py:175: DeprecationWarning: \u001b[33mWARN: Function `_bigint_from_bytes(bytes)` is marked as deprecated and will be removed in the future. \u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/buffers.py:219: UserWarning: This system does not have apparently enough memory to store the complete replay buffer 56.46GB > 11.44GB\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:43: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"render.modes\"] is marked as deprecated and will be replaced with `env.metadata[\"render_modes\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if distutils.version.LooseVersion(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/seeding.py:47: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:43: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"render.modes\"] is marked as deprecated and will be replaced with `env.metadata[\"render_modes\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if distutils.version.LooseVersion(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/seeding.py:47: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=157, episodic_return=1.0\n",
            "global_step=373, episodic_return=3.0\n",
            "global_step=488, episodic_return=0.0\n",
            "global_step=698, episodic_return=2.0\n",
            "global_step=813, episodic_return=0.0\n",
            "global_step=1001, episodic_return=2.0\n",
            "global_step=1191, episodic_return=2.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:43: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"render.modes\"] is marked as deprecated and will be replaced with `env.metadata[\"render_modes\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if distutils.version.LooseVersion(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/seeding.py:47: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=1306, episodic_return=0.0\n",
            "global_step=1465, episodic_return=1.0\n",
            "global_step=1654, episodic_return=2.0\n",
            "global_step=1769, episodic_return=0.0\n",
            "global_step=1956, episodic_return=2.0\n",
            "global_step=2194, episodic_return=3.0\n",
            "global_step=2382, episodic_return=2.0\n",
            "global_step=2541, episodic_return=1.0\n",
            "global_step=2910, episodic_return=6.0\n",
            "global_step=3069, episodic_return=1.0\n",
            "global_step=3323, episodic_return=4.0\n",
            "global_step=3465, episodic_return=1.0\n",
            "global_step=3622, episodic_return=1.0\n",
            "global_step=3735, episodic_return=0.0\n",
            "global_step=3894, episodic_return=1.0\n",
            "global_step=4055, episodic_return=1.0\n",
            "global_step=4168, episodic_return=0.0\n",
            "global_step=4279, episodic_return=0.0\n",
            "global_step=4390, episodic_return=0.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:43: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"render.modes\"] is marked as deprecated and will be replaced with `env.metadata[\"render_modes\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if distutils.version.LooseVersion(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/seeding.py:47: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=4533, episodic_return=1.0\n",
            "global_step=4648, episodic_return=0.0\n",
            "global_step=4834, episodic_return=2.0\n",
            "global_step=5051, episodic_return=3.0\n",
            "global_step=5212, episodic_return=1.0\n",
            "global_step=5327, episodic_return=0.0\n",
            "global_step=5466, episodic_return=1.0\n",
            "global_step=5625, episodic_return=1.0\n",
            "global_step=5836, episodic_return=2.0\n",
            "global_step=6094, episodic_return=3.0\n",
            "global_step=6253, episodic_return=1.0\n",
            "global_step=6392, episodic_return=1.0\n",
            "global_step=6628, episodic_return=3.0\n",
            "global_step=6743, episodic_return=0.0\n",
            "global_step=7030, episodic_return=4.0\n",
            "global_step=7143, episodic_return=0.0\n",
            "global_step=7283, episodic_return=1.0\n",
            "global_step=7568, episodic_return=4.0\n",
            "global_step=7726, episodic_return=1.0\n",
            "global_step=7889, episodic_return=1.0\n",
            "global_step=8048, episodic_return=1.0\n",
            "global_step=8189, episodic_return=1.0\n",
            "global_step=8304, episodic_return=0.0\n",
            "global_step=8687, episodic_return=6.0\n",
            "global_step=8893, episodic_return=2.0\n",
            "global_step=9077, episodic_return=2.0\n",
            "global_step=9313, episodic_return=3.0\n",
            "global_step=9567, episodic_return=3.0\n",
            "global_step=9753, episodic_return=2.0\n",
            "global_step=10017, episodic_return=4.0\n",
            "global_step=10173, episodic_return=1.0\n",
            "global_step=10358, episodic_return=2.0\n",
            "global_step=10471, episodic_return=0.0\n",
            "global_step=10634, episodic_return=1.0\n",
            "global_step=10822, episodic_return=2.0\n",
            "global_step=10937, episodic_return=0.0\n",
            "global_step=11094, episodic_return=1.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:43: DeprecationWarning: \u001b[33mWARN: `env.metadata[\"render.modes\"] is marked as deprecated and will be replaced with `env.metadata[\"render_modes\"]` see https://github.com/openai/gym/pull/2654 for more details\u001b[0m\n",
            "  logger.deprecation(\n",
            "/usr/lib/python3.10/subprocess.py:1796: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = _posixsubprocess.fork_exec(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/monitoring/video_recorder.py:421: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "  if distutils.version.LooseVersion(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/utils/seeding.py:47: DeprecationWarning: \u001b[33mWARN: Function `rng.randint(low, [high, size, dtype])` is marked as deprecated and will be removed in the future. Please use `rng.integers(low, [high, size, dtype])` instead.\u001b[0m\n",
            "  deprecation(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global_step=11233, episodic_return=1.0\n",
            "global_step=11421, episodic_return=2.0\n",
            "global_step=11564, episodic_return=1.0\n",
            "global_step=11725, episodic_return=1.0\n",
            "global_step=11840, episodic_return=0.0\n",
            "global_step=11951, episodic_return=0.0\n",
            "global_step=12110, episodic_return=1.0\n",
            "global_step=12283, episodic_return=2.0\n",
            "global_step=12442, episodic_return=1.0\n",
            "global_step=12555, episodic_return=0.0\n",
            "global_step=12668, episodic_return=0.0\n",
            "global_step=12783, episodic_return=0.0\n",
            "global_step=12942, episodic_return=1.0\n",
            "global_step=13106, episodic_return=1.0\n",
            "global_step=13291, episodic_return=2.0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-41-2a24d69601cd>\u001b[0m in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;31m# Here we store the transitions in D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0mrb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreal_next_obs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdones\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/stable_baselines3/common/buffers.py\u001b[0m in \u001b[0;36madd\u001b[0;34m(self, obs, next_obs, action, reward, done, infos)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdones\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_timeout_termination\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}